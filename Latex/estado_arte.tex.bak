\chapter[Estado del Arte]{\label{ch:estado-arte}Estado del Arte}


La regla del vecino más cercano fue originalmente propuesta por Cover y Hart \cite{hart1966asymptotic}, \cite{cover1967nearest} y a lo largo de las investigaciones está siendo utilizada por varios investigadores. Una razón para el uso de esta regla es su simplicidad conceptual, que conduce a la programación directa, si no necesariamente la más eficiente. Este algoritmo utiliza la idea central de la distancia euclidiana entre dos puntos en el espacio, en base a este cálculo se analiza la similud entre todos los puntos existentes.
En un artículo posterior, Hart \cite{hilborn1968dg} sugirió un medio de disminución de la memoria y los requisitos de computación. Este artículo introduce una técnica, la regla del vecino más cercano reducido, que puede conducir a más ahorros. Los resultados de esta regla se demuestran aplicándola a los datos de $"Iris"$ \cite{freeman1969experiments}.

El algoritmo k-NN \cite{knn1} \cite{knn2} tiene grandes requisitos de almacenamiento, pero pueden reducir significativamente el trabajo de aprendizaje de otro tipo de técnicas y aumentar la precisión en la clasificación. Uno de los problemas más grandes que presenta este algoritmo es el alto costo de procesamiento y memoria para revolver las consultas.\\

Se aplica ampliamente en el reconocimiento de patrones y en clasificación para minería de datos, debido a su simplicidad y baja tasa de error. Antes de la aparición masiva de plataformas paralelas, la realización por fuerza bruta (exhaustivamente) no se consideraba como una opción válida, especialmente para grandes bases de datos de entrenamiento y espacios de alta densidad. Para reducir el espacio de búsqueda y evitar tantos cálculos de distancia como sea posible, se han propuesto muchos enfoques de indexación. La mayoría de los métodos de recuperación se basan en kd-trees \cite{bentley1979data}. Hay una gran cantidad de trabajo sobre las adaptaciones de la estructura básica kd-tree para el problema k-NN \cite{brisaboa2008clustering} \cite{paredes2009egnat}. También se han propuesto estructuras no-tree que dividen eficientemente el espacio de búsqueda \cite{chavez2000effective} \cite{brisaboa2006similarity} .Las implementaciones basadas en MPI que utilizan estas estructuras también aparecen con mucha frecuencia en la literatura \cite{plaku2007distributed}.\\
La principal desventaja de estos métodos es que necesitan construir y mantener estructuras de datos complejas para el conjunto de datos de referencia. De manera que, k-NN se implementa típicamente utilizando métodos de fuerza bruta. 

Por lo tanto, las soluciones actuales de GPU han sido de alguna manera más simples y menos eficientes desde el punto de vista del algoritmo. Por ejemplo, Benjamin Bustos et al. \cite{bustos2006graphics} propuso una implementación no-CUDA explotar memorias de textura GPU. Su implementación sólo busca el elemento mínimo, lo que simplifica significativamente el problema.

El cálculo de la distancia exhaustiva en conjunción con la clasificación en paralelo se propuso en \cite{garcia2008fast}, \cite{kuang2009practical}. Vecente Garcia et al. \cite{garcia2008fast} propuso un orden de inserción paralelo modificado con el fin de obtener sólo los k elementos más cercanos, mientras que Kuang et al. \cite{kuang2009practical} propuso un mejor Radix-sort para realizar la clasificación final.

Además existe una versión de GPU de Quicksort que fue propuesta por Daniel Cederman y Philippas Tsigas \cite{cederman2009gpu}, que mostró ser más rápido que los algoritmos de clasificación anteriores. Este trabajo se basa en la propuesta que también calcula exhaustivamente todas las distancias, pero utiliza una metodología específica basada en heap para encontrar los k elementos más cercanos, propuesta por Ricardo Barrientos y José Gómez \cite{barrientos2010heap} 